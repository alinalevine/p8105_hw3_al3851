---
title: "p8105 HW 3"
author: "Alina Levine"
date: 2018-10-16
output: github_document
---

#Problem 1

```{r packages, results = "hide", message = FALSE, warning = FALSE}
library(p8105.datasets)
library(tidyverse)
library(ggplot2)
library(tidyverse)
library(scales)
library(ggrepel)
library(viridis)


knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)

```


##Data Cleaning

I am filtering to get just overall health questions and am ordering factor levels for the responses. Then I am renaimgin locationabbr "state".
```{r clean_brfss, results = "hide", warning = FALSE, message = FALSE}


brfss_df = brfss_smart2010 %>%
  janitor::clean_names() %>%
  filter(topic == "Overall Health") %>%
  filter(response %in% c("Excellent", "Good", "Poor", "Very good", "Fair")) %>%
  mutate(response = factor(response, levels = c("Excellent", "Good", "Poor", "Very good", "Fair"))) %>%
  rename("state" = locationabbr)


```

#Number Locations/state

I am grouping by state to get the number of location per state
```{r locations, results = "hide", warning = FALSE, message = FALSE}
locations_sum_df = brfss_df %>%
  group_by(state) %>%
  distinct(locationdesc, keep.all = TRUE) %>%
  summarize(n_locations = n())
  
filter(locations_sum_df, n_locations == 7)


```
CT, MT, NH, NM, OR, TN, and UT all had responses from 7 locations. 7 locations was the mode of the number of locations per state. 

#Spaghetti Plot

After I group by state and year, I am just keeping 1 locationdesc from each group so I can get the number of locationdesc per year per state.
```{r Spaghetti, results = "hide", warning = FALSE, message = FALSE}
locations_df = brfss_df %>%
  group_by(state, year) %>%
  distinct(locationdesc, .keep_all = TRUE) %>%
  summarize(n_observations = n()) 

ggplot(locations_df, aes(x = year, y = n_observations, color = state)) +
  geom_line() +
  labs(title = "Location Number in 2002-2010 by State",
       x = "Year", 
       y = "Number of Locations") +
  scale_color_hue(name = "State")
  


```

This spaghetti plot shows that Florida was an outlier in the number of locations in 2007 and 2010. During these years, there were huge jumps in the number of locations reporting in Florida.In 2007 there was a huge fall in the number of locations after the huge jump. 


#Excellent  Table

I am getting the mean excellent proportions and standard deviation of excellent proportions in 2002, 2006, and 2010
```{r Excellent Table,  warning = FALSE, message = FALSE}

brfss_excellent_tab = brfss_df %>%
  filter(year %in% c(2002,2006, 2010), state == "NY", response == "Excellent") %>%
  group_by(year) %>%
  summarize(mean = mean(data_value), sd = sd(data_value))

knitr::kable(brfss_excellent_tab)
  
```

The mean proportion of excellent responses decreased from 2002 to 2006, but then it was about the same in 2010 as in 2002. However, the standard deviation is larger than the difference in mean between 2002 and 2006. 




#Average Proportion

I am creating a dataframe that has the mean proportion for each response category grouped by year and state. When I plot these mean proportions with respect to year, I am rotating the axis labels, so the years do not intersect
```{r average proportion, results = "hide", warning = FALSE, message = FALSE}

brfss_prop_av = brfss_df %>%
  group_by(year, state, response) %>%
  summarize(mean_prop = mean(data_value, na.rm = TRUE))

ggplot(brfss_prop_av, aes( x = year, y = mean_prop, color = state)) +
  geom_line() +
  facet_grid(~response) +
  labs(title = "Mean Proportions by Category",
       x = "Year",
       y = "Mean Proportion") +
  scale_color_discrete(guide = FALSE) +
  scale_x_continuous(breaks = scales::pretty_breaks(n = 4)) +
  theme(panel.margin = unit(1, "cm")) +
  theme(axis.text.x = element_text(angle = 60, hjust = 1))



```
Scores at least as high as good were more common than fair and poor scores. Very good tended to be the most popular. Excellent and very good scores also had the most variability in mean proportion. Poor scores tended to have the least variability.


#Problem 2

```{r instacart, results = "hide", warning = FALSE, message = FALSE}

instacart_df = instacart

```

There are `r nrow(instacart_df)` rows and `r ncol(instacart_df)` columns. Each observation is identified by the columns order_id and product_id, so each observation is a particular order of a particular product. Some key variables give information about when an order was made. For example , there is a variable for the day of the week which has values 0-6 (sunday-saturday), the hour of the day, as well as the number of days since the last order, for each product in an order. Aside from information about order timing, there are variables about the product. For example, one variable gives the name of the product and another gives the department of the product. 

#Popular Aisles

I am filtering by aisle_id to get the number of items ordered in each aisle

```{r popular items, results = "hide", warning = FALSE, message = FALSE}


n_aisles = length(unique(instacart$aisle))

aisle_table = instacart_df %>%
  group_by(aisle_id) %>%
  mutate(n_items = n()) %>%
  distinct(aisle_id, .keep_all = TRUE)

aisle_table_order = aisle_table %>%
  arrange(desc(n_items))

  
ggplot(aisle_table, aes(x = aisle_id, y = n_items, color = department)) +
  geom_point(stat = "identity") + 
  geom_text_repel(data = head(aisle_table_order, 10), aes(x = aisle_id, y = n_items, label = aisle)) +
  theme(legend.position = "bottom") +
  labs(title = "Number of Items Per Aisle",
       x = "Aisle Number",
       y = "Number of Items")

  

```

There are `r n_aisles` aisles. The most popular item is `r aisle_table_order$aisle[1]`. On the aisle items plot, I labeled the most popular items, since too many labels would make the plot impossible to read. I also ordered by aisle number because there was no way to put aisle name on the axis, so at least there is a way to identify aisles that are not labeled. I colored by department. Departments containing the aisles with the most popular items were produce and dairy. 


#Popular Items

I am just including baking ingredient, dog food care, and packaged vegetables fruits aisles. The I am getting the product count for each product in each aisle and am only including the items that are the most popular in their respective aisle.

```{r item_count, warning = FALSE, message = FALSE}

item_count = instacart_df %>% 
  filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>%
  group_by(product_id) %>%
  mutate(product_count = n()) %>%
  ungroup() %>%
  group_by(aisle) %>%
  filter(product_count == max(product_count)) %>%
  distinct(product_id, .keep_all = TRUE) %>%
  select(aisle, product_name, product_count)

knitr::kable(item_count)

```


Organic baby spinach is the most popular item of these most popular items in these three isles with 9784 orders.

#Mean Hour of Orders

I am getting the mean hour of orders for both Pink Lady Apples and Coffee Ice Cream for each day of the week and then will compare them. 
```{r mean hour, warning = FALSE, message = FALSE}

mean_hour_df = instacart_df %>%
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>%
  group_by(order_dow, product_name) %>%
  summarize(mean_hour = mean(order_hour_of_day)) %>%
  ungroup() %>%
  mutate(order_dow = as.character(order_dow)) %>%
  mutate(order_dow = recode(order_dow, "0" = "Sunday", "1" = "Monday", "2" = "Tuesday", "3" = "Wednesday", "4" = "Thursday", "5" = "Friday", "6" = "Saturday")) %>%
  spread(key = order_dow, value = mean_hour) %>%
  select(product_name, "Sunday", "Monday", "Tuesday","Wednesday", "Thursday", "Friday", "Saturday", "Sunday")

knitr::kable(mean_hour_df)


```

The mean order hour for every day of the week was late morning to early afternoon. The mean hours are similar over the weekend for the two products but they differ by at least three hours on Monday, Tuesday, and Thursday. Coffee ice cream is ordered much later than Pink Lady Apple on those days.



#Problem 3


#ny_noaa Cleaning

I am converting tmin and tmax to Farenheit and am converting precipitation and snow to inches. I an also extracting the year and the month from the date. I will then use the apply function to get the proportion of missing values for each column.
```{r ny_noaa cleaning, results = "hide", warning = FALSE, message = FALSE}
ny_noaa_df = ny_noaa %>%
  mutate(year = lubridate::year(date),
         month = lubridate::month(date),
         day = lubridate::day(date)) %>%
  mutate(tmin = as.numeric(tmin) / 10 * (9 / 5) + 32, tmax = as.numeric(tmax) / 10 * (9 / 5) + 32) %>%
  mutate(snwd = (as.numeric(snwd) / 10) / 2.54, snow = (as.numeric(snow) / 10) / 2.54) %>%
  mutate(prcp = (as.numeric(prcp) / 100) / 2.54) %>%
  mutate(snow = ifelse(snow < 0, NA, snow))

apply(ny_noaa_df, MARGIN = 2, FUN = function(x){sum(is.na(x)) / nrow(ny_noaa_df)})

ggplot(ny_noaa_df, aes(x = snow)) +
  geom_histogram(bins = 500) +
  scale_x_continuous(limits = c(0, 50), breaks = scales::pretty_breaks(n = 15)) +
  scale_y_continuous(limits = c(0, 35000)) +
  labs(title = "NY Snowfall Distribution (1981-2010)")

  


```
The ny_noaa dataframe is `r nrow(ny_noaa_df)` rows by `r ncol(ny_noaa_df)`. After converting units, the key variables are the station identification number, the date the weather was recorded, the precipitation (inches) recorded, snow depth (inches) recorded, snowfall recorded (inches), minimum temperature(degrees farenheit) and maximum temperature (degrees farenheit). The fact that there is so much missing data may negatively affect any analysis, since so many stations will be unrepresented in plots and calculations for estimates. Temperatures are missing in 44 percent of the stations and snow depth is missing in 23 percent. If it happens to be that weather stations in certain parts of NY, such  are as upstate NY, are more likely not to report, the distribution of the variables will not be representative of the state. As shown in the histogram, snowfalls with values between less than one inch are most common. For large snowfalls, there has to be the combination of a strong storm and freezing temperatures, so small snowfalls or no snowfalls at all are much more common than larger ones. It must also be noted that snowfall has one negative value which makes no sense, so I am recoding that to NA. 


#Average Maximum Temperatures

I am grouping by station id, year, and month to get the average maximum temperature for each station in each year in both the months July and January
```{r temp by station, results = "hide", warning = FALSE, message = FALSE}

average_max_df = ny_noaa_df %>%
  filter(month %in% c(1,7)) %>%
  select(id, month, year, tmax) %>%
  group_by(id, year, month) %>%
  summarize(average_max = mean(tmax, na.rm = TRUE)) %>%
  mutate(month = as.character(month)) %>%
  mutate(month = recode(month, "1" = "January", "7" = "July"))

ggplot(average_max_df, aes(x = year, y = average_max, color = month)) +
  geom_point() +
  scale_y_continuous(breaks = scales::pretty_breaks(n = 10)) +
  facet_grid(~month) + 
  labs(title = "Average Max Temperature by Station from 1981-2010",
         y = "Average Max Temp (Degrees F)") +
  theme(panel.margin = unit(1, "cm"))

  
    

```


It appears as though average maximum temperatures in January tend to have a larger range each year than average maximum temperatures in July. There is more variability in the average maximum temperatures. It looks like there are chuncks of time where changes in average monthly temperature follow the same pattern in July in August, but there are other chuncks where this is not the case (i.e some years values go up and down together but other times they don't ),so it is unclear whether average max temperature in July and June go hand in hand. There are several outliers over the years. In 1982 there was a station about 7 degrees colder than the next lowest station. In 1888 the average max temperature in July was 7 degrees colder than the next highest. It's important to note that there are 5970 observations that are missing. 

#Maximum vs. Minimum Temperatures

```{r t max min, results = "hide", warning = FALSE, message = FALSE}

ggplot(ny_noaa_df, aes(x = tmin, y = tmax)) +
  geom_hex() +
  labs(title = "Minimum vs. Maximum Temp from 1980 to  2010",
       x = "tmin (Farenheit)",
       y = "tmax (Farenheit)") + 
  scale_fill_viridis()


```


As expected, t_max is positively associated with t_min. The relationship between t_max and t_min looks approximately linear. The most densly distributed minimum temperatures occur between about 10 and 60 degrees Farenheit and the most densly distributed maximum temperatures occur between 25 and 80 degrees Farenheit. Some areas of density do not make sense. For example, there is one hexagon where minimum temperature is around -10 degrees Farenheit and the maximum temperature is about 135 degrees Farenheit. I searched for this observation or observations in the dataframe, and a situation like this was recorded on January 28, 2005. How is it possible that a station became so hot and so cold on this day in January?

#Snowfall
```{r snowfall, results = "hide", warning = FALSE, message = FALSE}
ny_noaa_df %>%
  filter(!is.na(snow)) %>%
  filter(snow > 0 & snow < 100) %>%
  ggplot(aes(x = year, y = snow)) +
  geom_hex(bins = 80) +
  labs(title = "Snowfall between 1981 and 2010",
       x = "Year",
       y = "Snowfall (inches)")

  
```


I chose to use geom_hex for this plot as well because it shows the large concentration of very small snowfalls. With geom_point, snowfalls of 10 inches looked just as common as snowfalls very close to zero inches. In this plot I made the bins a smaller size, so that years would be separted. This plot shows that snowfalls above 40 inches were rare. 
